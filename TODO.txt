Hey team. Here are the different parts we need to handle for the project

1: environment set up 
    -use virtualenv? -> locally select for each machine
    -get a requirements file -> one has been started
    -.sh scripts to load everything -> we are manually loading data into google cloud
    -need to figure out pytorch vs tf, pickle, numpy, logger -> pytorch, still use tf in logger 
    
2: data download/processing/cleaning
    - write .sh script to download yelp photo data using wget? -> nope, manually download and upload to GC
    - reduce entire JSON data to just photos and yelp ratings -> done in dataset.py
    - write a loader class, needs to follow data_loader API for pytorch -> done in dataset.py
    - train/val/test set -> done in dataset.py

3: model building, classifier
    - start with this
    - use GoogleNet/ResNet pretrained layers for speed/accuracy
    - do architecture surgery on last few layers for yelp review
    - begin classifying, tune hyperparameters

4: model building, GAN
    - honestly not sure what this entails
    - but y'all do and I will soon!

5: testing / analysis
    - how well can we predict yelp scores with thorough hyperparameter tuning?
    - what do fail cases look like? 
    - GAN stuff idk

